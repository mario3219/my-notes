<u>For Image2Count</u>
* Create model script for Image2Count to import
* (DONE) Find out how image data is being used in GAT step
* (DONE) Find out how the GAT network leads to ROI prediction

<u>For foundation model</u>
* Find out how multi-plex models output input work

<u>Other</u>
* csv generator
* Torch geometric

**Find out how GAT step works**
****

Preprocess in src.run.utils.image_preprocess, where cull_cutouts are made and mean and std are saved as npy files.

Visual feature model is trained. In the same step, src.run.CellContrastEmbed function embed is called to form the embeddings using the trained model

CellContrastEmbed first loads the trained visual extractor model. Then, src.data.CellContrastData, EmbedDataset gets created, which when initialized creates and saves kmeans cluster centers in kmeans_cluster_centers.npy by loading the .npy files from the preprocessing step. The constructor also creates a transform attribute that introduces flips, background, jitter and artefacts if wanted.

The EmbedDataset then gets passed through the model using EmbedDataset.save_embed_data and forms the embeddings for the GAT network, saved as cell paths concatenated with 
$\textunderscore$embed.pt.

The embeddings are formed and that dataset is associated with its embedding by just forming the output when passed through the model, and is being tracked by having the original cell cut-out path concatenated with $\textunderscore$embed.pt.

Training the GAT is then done in src.run.GraphTrain

It creates datasets using src.data.GeoMxData

Inside GeoMxData class, a new representation of data is formed, using the torch_geometric library. The constructor loads relevant npy files like the embeddings from before, the ROIs stated in label_data (?) and the information for images.

The dataset class initializes transforms for KNN, distance and local cartesian, which are other data types.

Subgraphs are created using $\textunderscore$create_subgraphs, which takes data from the processed file names, training map and validation map, IDs and unique IDs.

The maps are arrays ranging from zero to the total amount of patient IDs.

IDs are extracted from the cells_embet.pt from before by extracting from ROI (?)

Data is an array of file names, maps are binary if ROI is used for training or validation. The function returns new data and maps pointing to the new subgraphs.

The subgraphs are formed by enumerating the data, and for each element in data use the associated graph_path. The data variable got processed using property function processed_file_names, which appended graphs to the structure.

The graphs in each iteration is formed by first loading the graph for that (??) case and calculating step sizes in x and y dimension. From this, points are generated for that (??) case. These points are stored as new datasets containing these newly formed subgraphs, and their associated cell expressions, sum of expressions, and saved.

With the dataset class initialized, accompanying functions are returned aswell that allows transformation (intoducing pixel jitter, distance calculations and knn) aswell as creating the cell graphs of an ROI, and also a get function which returns the graph, and an embed function that loads the graph, predicts single cell expressions using the trained GAT, and forms the ROI expression by summarizing the expressions from the single cell expressions. There is also an embed function that saves all predictions in ROI and cell pred files.

Back to the GraphTrain, where the GAT model is trained, train dataset gets initiated with k fold, and dataloaders get initialized. Then the model to train gets initialized, using src.models.GraphModel class ROIExpression

The ROIExpression class has attributes number of GAT blocks (described as layers), number of node features, number of embedding features, nnumber of edges, attention heads, dropout rates for embed and convolutional layers. The class initializes a gnn corresponding to the parameters used, a pooling layer, a projectionhead (which seems to be a linear layer with a bias relu atctivation layer), a normalization layer and a mean act (initialized in forward function as torch.clamp and exponents)

The corresponding forward function returns either the clamp function result or result after a pooling layer.

Back in GraphTrain, AdamW is used as optimizer, L1 loss, and cosine similarity is used.

In training, gradient for loss is calculated, backwards propagated using the gradients of the loss, and optimizer takes a step.

k fold is made by having a big outer loop deciding which fold to use for current training iteration

Trained model is saved.

<u>Note</u>: finding out where cell graphs are made needs to be done by finding where process function gets called
ANSWER: in GeoMx data, a path to processed data is trying to be found. If found, it is left as is and just returns the paths. If the path doesn't exist, the function calls process()

**$\textunderscore$process_one_step, the cell graph function**
The script loads cell positions and the cell embeddings from the visual model. Counts are initialized as zero, an annotation data set is initialized with same shape as counts and with the annotated coordinates of cells. Squipy spatial neighbors dataset is initialized, which is a spatial neighborhood dataset that is a spatial neighborhood graph with counts and distances. Passing knn as argument leads to the data prioritizing the closest neighbors instead of distances in internal logic.

the adata and spatial neighborhood graph are coupled into a dataset, and only euclidian distance as edges gets exported to a new data set edge_matrix, and further extracted edge_index and edge_attri instead of using the matrix

Labels, being the ROI corresponding to the current cell embedding, gets extracted using file_index. A new array corresponding to column four and onwards are extracted (count columns) 

All of this data gets stored using torch.geometric Data.

**Embed gnn data or graph train data** 
Embeds predicted cell expression of test data. Done through src.run.GraphEmbed, embed. Wether this function is called depends on gnn_data argument if its used or not, but it needs to be present if graph_train_data wants to be used. The graph_train_data chooses which split to be embedded.

The embed function in GraphEmbed calls embed in GeoMxData class, which loads the graph, predicts using that graph the cell prediction, and summarizes expressions for all those cells to create the ROI prediction, and saves the results.

So, the embeddings are the graphs, passed through the GAT, and forms outputs.

So GraphEmbed class is where the prediction happens?

<u>subgraphs</u>
Created from graphs, allows more batches to train against. The create_subgraphs calculates the amount of steps to be made according to the smallest and largest coordinates (because if the big graph in the ROI has smalls that are close, the stepsize will be smaller)

<u>Question</u>: 
* what is actually enumerated in geoMx?
* Where did the cell expression get associated with the graph?
* In processed_file_names, graph_...pt gets loaded, when were these formed?
* What are subgraphs?
* src.models.GrapModel MeanAct(), what does the clamp do? (think it was mentioned in methodology)
* Single cell cutouts are made, how are we training with GeoMx data? Because in my mind, geomx data are for ROIs
* Is kfold made in splits on patient IDs? (says total samples is defined as unique IDs)
* My understanding is we are not getting expressions for each cell to train against, rather we have a collected bunch of vectors, representing each cell, that are associated with an ROI.

****

<u>Questions</u>
* Three different models can be initialized in GraphTrain,
	* NIL
		A model to compare to for performance review, just a linear layer
	* Image2Count
		The main model
	* IMAGE
		Prototyping model, see if predicting head separately would be better or together
	What are these?

