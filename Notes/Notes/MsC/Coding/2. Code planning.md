<u>For Image2Count</u>
* Create model script for Image2Count to import
* Find out how image data is being used in GAT step
* Find out how the GAT network leads to ROI prediction

<u>For foundation model</u>
* Find out how multi-plex models output input work

<u>Other</u>
* csv generator
* Torch geometric

**Find out how GAT step works**
****

Preprocess in src.run.utils.image_preprocess, where cull_cutouts are made and mean and std are saved as npy files.

Visual feature model is trained. In the same step, src.run.CellContrastEmbed function embed is called to form the embeddings using the trained model

CellContrastEmbed first loads the trained visual extractor model. Then, src.data.CellContrastData, EmbedDataset gets created, which when initialized creates and saves kmeans cluster centers in kmeans_cluster_centers.npy by loading the .npy files from the preprocessing step. The constructor also creates a transform attribute that introduces flips, background and artefacts if wanted.

The EmbedDataset then gets passed through the model using EmbedDataset.save_embed_data and forms the embeddings for the GAT network, saved as cell paths concatenated with 
$\textunderscore$embed.pt.

The embeddings are formed and that dataset is associated with its embedding by just forming the output when passed through the model, and is being tracked by having the original cell cut-out path concatenated with $\textunderscore$embed.pt.

Training the GAT is then done in src.run.GraphTrain

It creates datasets using src.data.GeoMxData

Inside GeoMxData class, a new representation of data is formed, using the torch_geometric library. The constructor loads relevant npy files like the embeddings from before, the ROIs stated in label_data (?) and the information for images.

The dataset class initializes transforms for KNN, distance and local cartesian, which are other data types.

Subgraphs are created using $\textunderscore$create_subgraphs, which takes data from the processed file names, training map and validation map, IDs and unique IDs.

The maps are arrays ranging from zero to the total amount of patient IDs.

IDs are extracted from the cells_embet.pt from before by extracting from ROI (?)

Data is an array of file names, maps are binary if ROI is used for training or validation. The function returns new data and maps pointing to the new subgraphs.

The subgraphs are formed by enumerating the data, and for each element in data use the associated graph_path. The data variable got processed using property function processed_file_names, which appended graphs to the structure.

The graphs in each iteration is formed by first loading the graph for that (??) case and calculating step sizes in x and y dimension. From this, points are generated for that (??) case. These points are stored as new datasets containing these newly formed subgraphs, and their associated cell expressions, sum of expressions, and saved.

Question: what is actually enumerated? Where did the cell expression get associated with the graph? In processed_file_names, graph_...pt gets loaded, when were these formed? What are subgraphs?

* EmbedDataset processes data by 

* How the datasets get loaded
Loaded through src.data.GeoMXData, GeoMXDataset
* How the model works
Model used is src.models.GraphModel, class ROIExpression

****

<u>Questions</u>
* Three different models can be initialized in GraphTrain,
	* NIL
		A model to compare to for performance review, just a linear layer
	* Image2Count
		The main model
	* IMAGE
		Prototyping model, see if predicting head separately would be better or together
	What are these?

