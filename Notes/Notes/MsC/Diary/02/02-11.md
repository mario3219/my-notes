The inputs work, the journey has come to an end. But the war is not over. My next journey embarks, to properly employ the encoder.

* Batch sizes
There is a set amount of data. The batch size decides how much of this data passes through the model at a time, so there will be a set amount of batches.
`num_batches = (data.shape[0] // batch_size) + 1`

* Embed size
Decides the size of the output of the embedding model. A larger embed size allows more defined structure, but a smaller embed size a more compact definition.

****
<u>Why the inputs_app shape changes</u>
It extracts from data using different indexes depending on if the loop has reached the last index in save_embed_data. If it has reached the last index, then it extracts all the last few samples, which can be less than the embedding size. Question is, why aren't the shapes consistent?

A qualifier is that the amount of batches should be divisible.

I guess it wasn't a problem before, because the embeddings would just be smaller for some parts, but not dangerous in the graph network?

Each cell.npy have different sizes.

The image encoder is made to handle multiple channels, and the embedding shape just happens to fit perfectly with save_contrast_data embed, continue like this?

What is required in the dataloader is the channel name mapping. it also pads images. converts to tensors