![[Pasted image 20260209121750.png]]

The journey of initializing the image encoder continues

I managed to load the image encoder with the weights, and separate the image feature extractor, although the weights of the image encoder had to be loaded first, so I'm doing two steps. Gonna see if I can load just the feature extractor.

Nevermind, we wanted the image encoder

Gonna see how the inputs and outputs matchup

****

<u>CellTypeDataEncoder</u>
The architecture uses a convolution block for image features, and a language encoder. The outputs of both get passed through a multi head attention block to form new outputs.

**forward**

line 187: Extract visual features
line 190:
* inputs_ch_names
(eg, CD3, CD20, PanCK)
Gives semantic meaning to each channel
line 198: Fuses image identity and visual features
line 201: Forms output with both in mind
line 204: Add CLS tokens
line 208:
Apply attention, each marker has a relevance score to each other, and embeddings are now weighted mixtures of each other
line 214-215: Apply classification on embeddings

Total return:
* Cell type logits
Prediction for celltype
* Domain logits
Prediction for domain
* CLS tokens for each embedding

cls_token_embedding encodes all the features.

**Inputs**
inputs_app
`inputs_app: (B,C,3,H,W)`
B, batch size
C, number of markers/channels
3, RGB
H,W, spatial size of each image

```
cell i
 ├─ CD3 image   (3 × H × W)
 ├─ CD20 image  (3 × H × W)
 ├─ PanCK image (3 × H × W)
 └─ ...
```

inputs_ch_names
```
CD3 : 0, CD20 : 1, PanCK : 2
```

inputs_ch_padding_masks
Missing marker mask
Padding masks tells attention not to look at this token
```
[False, False, True, False]
>> Marker 3 is missing
```

****
<u>Questions for Markus</u>
* Make save_embed_data model dependent (?)
The models have different inputs for forward calls, easier if each model has their own save_embed_data
* Include a pre_processing step for the cls tokens