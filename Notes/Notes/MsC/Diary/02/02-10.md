The journey to make the inputs work continues

Current problem:
Shape changes in the forward call, at line 296. The embeddings stay at shape 256, but the x changes shape to 89, and its not because of the model, but the inputs themselves change in sizes.

Im gonna try by forcing the shape to be 256 at all times and see if the code still runs.

It works! But i need to figure out why the shapes has to be consistent

The marker embedding layer is initialized using a config, which is retrieved from dct_config.get_channel_embedding, and takes an embedding model name.

So, the embeddings for channels are precalculated from a .json file, and the model name decices which model we pick our embedding output from.

The language model was set to embed in size 256 for each marker, and these embeddings are summarized with the image embeddings so they can be passed to the transformer block, thats why its required for them to share shape

The input_channel_names are used in the forward call in the embedding layer.

<u>Question:</u> How does it know which markers im using?
It happens because of embed_layer in MarkerNameEmbeddingLayer, the forward call takes an index

****
Markus suggested for me to implement their dataloader and workflow in the tutorial.

<u>In predict.py</u>

What is different in deepcell compared to I2C is that they have raw images, and a mask to show where the cells are, while in I2C, we use qupath.
The masks are used to create cell masks, then referenced using cell indexes, and attention masks.

The dataloader takes raw images, masks, channel names, mpp and dct_config. The images are converted to numpy and filtered from channels not mentioned in the config. The raw images are passed through a patch generator (HERE ALOT OF PREPROCESSING IS DONE FOR THE IMAGE) and ultimately makes crop outs of the image of cells.

The save_embed function uses cell.npy tensors.

<u>Questions</u>:
* Images are numpy arrays in deepcell, are tensors transferable?
* The dataloader is used to return samples (cell cutouts), channel indexes, attention masks and cell indexes. I think we can skip the data loader, as the samples are already provided through qupath. The preprocessing however needs to be done still.
* Preprocessing assumes it is done on full image, but we made cutouts in qupath. What do you suggest? Line 137 and 140 seem to be problematic if used in cutouts.
I suggest using a script that preprocesses the full images, THEN we do qupath cell cutouts.
Or maybe use one image as the standard.
They are extracting max and min from the image, maybe pass a global max min from all images.
* The forward for the image encoder is supposed to take one channel at a time, should we make it be able to take all channels, and return a larger tensor with this extra dimension?
* The embeddings in save_embed_data, at this point must be able to handle several channels, does it do that? Does GAT also have to be changed to handle this?