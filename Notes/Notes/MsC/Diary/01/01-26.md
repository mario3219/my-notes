I've learned more about how to use the models, specifically:
* How to remove the last layer, to get just the visual features
* What the input types and output types are
* How to transform the inputs to work with the models

It seems however it is case dependent, depending on the model

I've also looked at the current structure of the I2C workflow.
I2C iterates using pytorch DataLoader, which is a dataprocesser with a dataset class as an input. The current dataset class does processing and forms the dataset using the user input, but more importantly, the visual feature backbone takes pytorch tensors that are converted from images.

In conclusion, the tensors can be transformed to work with ViT directly

So, the input to the visual feature backbone seems to be possible to implement currently.

Next step is to look if the output embeddings will be challenging to implement into the GAT model.

I got notified that the data is multiplex, so I need to learn about how the foundation models handle this.