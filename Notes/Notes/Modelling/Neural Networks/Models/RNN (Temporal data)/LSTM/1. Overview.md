Long Short-term memory is a RNN that contains memory blocks. Each memory block contains memory cells and three multiplicative units:
* Input gate
* Output gate
* Forget gate
which are nonlinear summation units and are in control of the activation of the cell. The memory blocks replace the summation units that exists in hidden layers in normal RNN networks.

**Principle added to RNN networks**
![[Pasted image 20260108223419.png]]

![[Pasted image 20260108223641.png]]

So instead of the left (RNN) which is the summation unit in the layer, the right (LSTM) is used.

![[Pasted image 20260108223721.png]]

What happens:
* Before everything starts, long-term gets scaled, essentially forget part of past memory
* (Blue) Short term mixes with input, passes through the activation function, and gets added to long term
* (Green and yellow) Short term and input calculates weights for how much of input and short-term to be stored in long term
* (Pink) Short term and input mix to be ready for cyan
* (Cyan) long term and short term gets compared, to be the new short-term