**Positional encoding**
Recursive algorithms does not work with parallel processing. Position of words are important, as different order of words carry different meanings.

Words are weighted with cos+sin functions, words with similar values carry similar weights.

**Self-attention**

![[Pasted image 20260108234412.png]]

For each word, independently calculate the different key values, which can be done in parallel. These values then form scalar products with other key values associated with the other words. Using a softmax function, percentages are formed which decide the importance of the word.

**Multi-headed self-attention**

![[Pasted image 20260108234602.png]]

