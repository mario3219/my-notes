A problem with seq2seq: entire input is compressed into one context vector, which is difficult for longer phrases.

LSTM was used to retain importance of some tokens, but a token at the beginning of a sentence needs to survive iteration until the end of the sentence. Attention solves this in a new way.

Every token is compared with every token, and an attention percentage is formed.

For each token independently, different keys are produced:
* Query
* Key
* Value
These values associated with each token is compared with every other token, and builds a score. These scores are used to form the attention weights for each token, using softmax. At the end what is formed, is how important each token is to every token in a word.

The result is *for this token, which other tokens should I read from to update myself?*
which in turn leads to routing of information from relevant words, to where it's needed.



