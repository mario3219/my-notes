A development of word embedding. The number of nodes in the NN decides the amount of dimensions, so each layer can learn different dimensions.

![[Pasted image 20260108232229.png]]

The network uses long- and short term memory, so past tokens can carry importance for the whole meaning behind the sentence.

**Translator**

![[Pasted image 20260108232243.png]]

The output of the context vector is used inside the decoder, which correlates equal contexts to words in another language. 

So each tokenized word gets put through their own FFW chain, and inputted into the next word chain (note the pink arrows) each block represents processing of each token inside the word. It is the token that gets put into memory. These memory operations influence the characteristics behind the word.