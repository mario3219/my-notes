**Bagging**
The bagging idea is to create multiple slightly different versions of training data, by randomly sampling overlapping subsets of data. Each model is then trained on the sub-data.
Variance gets reduced, as the whole data isn't used for one singular model. Risk of overfitting decreases.

Sampling is done with replacement.