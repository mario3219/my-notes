**ADAM**
Stores past computations of gradients, makes learning rate adaptive. The moments, which are derivations of the derivatives, is part of the correction coefficient for the update function. The function emphasizes recent gradients more than present ones.

**RMSProp**
Short for root mean square propagation, averages of first moments