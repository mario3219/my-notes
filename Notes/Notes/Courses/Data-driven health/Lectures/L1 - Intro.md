![[Pasted image 20260106150507.png]]
**Generalization gap**
How big the gap is between training error and validation error

If the gap is large, means overfitting
If the gap is small, then its optimal

If the training error is large, then the model isn't complex enough, which
means underfitting

**Baseline**


Another way is the *achievable performance*. You compare with current
models and their accuracy.

**The different model approaches**

![[Pasted image 20260106151426.png]]

